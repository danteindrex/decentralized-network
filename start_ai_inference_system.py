#!/usr/bin/env python3
"""
Start AI Inference System with Live Bootstrap Node
Configures and starts all components for AI inference
"""

import os
import json
import time
import subprocess
import requests
import sys
from pathlib import Path

# Configuration
BOOTSTRAP_NODE = "https://bootstrap-node.onrender.com"
BOOTSTRAP_IPS = ["35.160.120.126", "44.233.151.27", "34.211.200.85"]
IPFS_HOST = "192.168.1.103"
IPFS_PORT = 5001

def check_services():
    """Check if required services are running"""
    print("üîç Checking services...")
    
    # Check bootstrap node
    try:
        response = requests.get(f"{BOOTSTRAP_NODE}/health", timeout=5)
        if response.status_code == 200:
            data = response.json()
            print(f"‚úÖ Bootstrap node: {data['status']} ({data['peers']} peers)")
        else:
            print("‚ùå Bootstrap node not responding")
            return False
    except Exception as e:
        print(f"‚ùå Bootstrap node error: {e}")
        return False
    
    # Check IPFS
    try:
        url = f"http://{IPFS_HOST}:{IPFS_PORT}/api/v0/version"
        response = requests.post(url, timeout=5)
        if response.status_code == 200:
            print(f"‚úÖ IPFS running: {response.json()['Version']}")
        else:
            print("‚ùå IPFS not running")
            return False
    except:
        print("‚ùå IPFS not accessible")
        return False
    
    # Check Ethereum node
    try:
        response = requests.post(f"http://{IPFS_HOST}:8545", 
            json={"jsonrpc":"2.0","method":"eth_blockNumber","params":[],"id":1},
            headers={"Content-Type": "application/json"})
        if response.status_code == 200:
            print("‚úÖ Ethereum node running")
        else:
            print("‚ùå Ethereum node not running")
            return False
    except:
        print("‚ùå Ethereum node not accessible")
        return False
    
    return True

def update_orchestrator_config():
    """Update orchestrator config with bootstrap node info"""
    print("\nüìù Updating orchestrator configuration...")
    
    config_path = Path("orchestrator/config.yaml")
    if not config_path.exists():
        print("‚ùå orchestrator/config.yaml not found")
        return False
    
    # Read current config
    import yaml
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # Update bootstrap nodes
    config['network']['bootstrap_nodes'] = [
        BOOTSTRAP_NODE,
        f"http://{BOOTSTRAP_IPS[0]}:8545",
        f"http://{BOOTSTRAP_IPS[1]}:8545",
        f"http://{BOOTSTRAP_IPS[2]}:8545"
    ]
    
    # Update deployment info if available
    deployment_path = Path("deployment.json")
    if deployment_path.exists():
        with open(deployment_path, 'r') as f:
            deployment = json.load(f)
            config['contract_address'] = deployment.get('inferenceCoordinator', config.get('contract_address'))
            config['model_registry_address'] = deployment.get('modelRegistry', config.get('model_registry_address'))
            config['node_profile_registry_address'] = deployment.get('nodeProfileRegistry', config.get('node_profile_registry_address'))
    
    # Write updated config
    with open(config_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False)
    
    print("‚úÖ Orchestrator config updated")
    return True

def update_streamlit_env():
    """Create .env file for Streamlit app"""
    print("\nüìù Creating .env for Streamlit...")
    
    env_content = f"""# AI Inference Network Configuration
ETH_NODE=http://{IPFS_HOST}:8545
IPFS_HOST={IPFS_HOST}
IPFS_PORT={IPFS_PORT}
BOOTSTRAP_URL={BOOTSTRAP_NODE}

# Contract addresses (from deployment.json)
"""
    
    # Add contract addresses if available
    deployment_path = Path("deployment.json")
    if deployment_path.exists():
        with open(deployment_path, 'r') as f:
            deployment = json.load(f)
            env_content += f"CONTRACT_ADDRESS={deployment.get('inferenceCoordinator', '')}\n"
            env_content += f"MODEL_REGISTRY_ADDRESS={deployment.get('modelRegistry', '')}\n"
            env_content += f"NODE_PROFILE_REGISTRY_ADDRESS={deployment.get('nodeProfileRegistry', '')}\n"
    
    # Write .env file
    with open(".env", "w") as f:
        f.write(env_content)
    
    print("‚úÖ .env file created")
    return True

def start_worker_node():
    """Start the vLLM worker node"""
    print("\nüöÄ Starting vLLM worker node...")
    
    # Check if vLLM is installed
    try:
        import vllm
        print("‚úÖ vLLM is installed")
    except ImportError:
        print("‚ùå vLLM not installed. Installing...")
        subprocess.run([sys.executable, "-m", "pip", "install", "vllm"], check=True)
    
    # Register worker with bootstrap node
    worker_data = {
        'nodeId': 'vllm-worker-001',
        'nodeType': 'worker',
        'endpoint': f"{IPFS_HOST}:8000",
        'capabilities': {
            'gpu': False,  # Would check torch.cuda.is_available() if torch installed
            'models': ['deepseek-1b'],
            'maxJobs': 5,
            'vllm': True
        }
    }
    
    try:
        response = requests.post(
            f"{BOOTSTRAP_NODE}/peers/register",
            json=worker_data
        )
        if response.status_code == 200:
            print("‚úÖ Worker registered with bootstrap node")
        else:
            print(f"‚ö†Ô∏è  Worker registration failed: {response.text}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not register worker: {e}")
    
    # Start the orchestrator main.py
    print("\nüöÄ Starting orchestrator (vLLM worker)...")
    os.chdir("orchestrator")
    subprocess.Popen([sys.executable, "main.py"])
    os.chdir("..")
    
    return True

def start_streamlit_app():
    """Start the Streamlit app"""
    print("\nüåê Starting Streamlit app...")
    subprocess.Popen(["streamlit", "run", "streamlit_app.py"])
    return True

def upload_deepseek_model():
    """Upload DeepSeek model if not already uploaded"""
    print("\nüì§ Checking DeepSeek model...")
    
    # Check if model is already registered
    model_cid = "QmVyvJ3BUuz1KiFidCHCKN2ZNJkt2dNWREYuyn4AJSnu6Q"
    
    # Check if we need to download the model
    model_path = Path("models/deepseek-1b")
    if not model_path.exists():
        print("üì• Downloading DeepSeek-1B model...")
        model_path.mkdir(parents=True, exist_ok=True)
        
        # Create a simple config.json for testing
        config = {
            "architectures": ["DeepSeekForCausalLM"],
            "hidden_size": 2048,
            "intermediate_size": 5504,
            "max_position_embeddings": 2048,
            "model_type": "deepseek",
            "num_attention_heads": 16,
            "num_hidden_layers": 24,
            "vocab_size": 100000
        }
        
        with open(model_path / "config.json", "w") as f:
            json.dump(config, f, indent=2)
        
        print("‚úÖ Created model config (full model download would happen here)")
    
    print(f"‚úÖ Model ready with CID: {model_cid}")
    return model_cid

def print_access_info():
    """Print access information"""
    print("\n" + "="*60)
    print("üéâ AI Inference System Started!")
    print("="*60)
    print("\nüìç Access Points:")
    print(f"   - Streamlit UI: http://localhost:8501")
    print(f"   - vLLM API: http://localhost:8000")
    print(f"   - Bootstrap Node: {BOOTSTRAP_NODE}")
    print(f"   - IPFS Gateway: http://{IPFS_HOST}:8081")
    print("\nüí° Test the system:")
    print("   1. Open Streamlit UI in your browser")
    print("   2. Select the DeepSeek-1B model")
    print("   3. Enter a prompt and submit")
    print("   4. Watch the inference happen!")
    print("\nüõë Press Ctrl+C to stop all services")

def main():
    print("ü§ñ AI Inference System Startup")
    print("="*60)
    
    # Check services
    if not check_services():
        print("\n‚ùå Required services not running!")
        print("Please ensure:")
        print("  - IPFS daemon is running")
        print("  - Ethereum node is running")
        return
    
    # Update configurations
    update_orchestrator_config()
    update_streamlit_env()
    
    # Upload model if needed
    model_cid = upload_deepseek_model()
    
    # Start services
    print("\nüöÄ Starting services...")
    
    # Start worker node (orchestrator with vLLM)
    start_worker_node()
    time.sleep(5)  # Give worker time to start
    
    # Start Streamlit app
    start_streamlit_app()
    time.sleep(3)  # Give Streamlit time to start
    
    # Print access info
    print_access_info()
    
    # Keep running
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\nüõë Shutting down...")

if __name__ == "__main__":
    main()